{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "%run \"../config.py\" # this imports variables from config.py as global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_column_name = 'text'\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ngram_range = (1, 2)\n",
    "min_df = 5 # cut-off value for ignoring rare words\n",
    "max_df = 1.0\n",
    "max_features = 500\n",
    "vectorizer = CountVectorizer(encoding='utf-8',\n",
    "                             ngram_range=ngram_range,\n",
    "                             stop_words=stop_words,\n",
    "                             max_df=max_df,\n",
    "                             min_df=min_df,\n",
    "                             max_features=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26122, 2)\n",
      "                    id                                               text\n",
      "0  1236315739428192259  “ May I get your name? Mine is 8... “\\n\\nShe w...\n",
      "1  1236315739667279872  MOVE AUBA IN THE MIDDLE AND BRING ON NELSON/MA...\n",
      "2  1236315739713187842  @giantlittleman Im so happy for u yet so jealo...\n",
      "3  1236315739805618183                                   Call me ice baby\n",
      "4  1236315739709210625  @ZonePhysics I wonder what the CO2 count is, h...\n",
      "5  1236315739512016897  @GWRHelp It was booked in advance, to confirm ...\n",
      "6  1236315739436593153  Jed-Forest v Hawick Rugby Live Stream Iphone 7...\n",
      "7  1236315739675557889  @Cyal8er3 @Beeeelzebub888 @UnCastellsMes @badi...\n",
      "8  1236315739939909634  Dr. Wilson came and got my baby for his circum...\n",
      "9  1236315739931492352  @cmclymer @MsNebraskaJones We have videos of h...\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "if data_filepath.endswith('.json'):\n",
    "    data_arr = []\n",
    "    column_names = ['id', text_column_name]\n",
    "    with open(data_filepath, 'r') as f:\n",
    "        for tweet in f:\n",
    "            selected_row = []\n",
    "            json_tweet = json.loads(tweet)\n",
    "            try:\n",
    "                # filter out retweets and non-English tweets\n",
    "                if not json_tweet['retweeted'] and 'RT @' not in json_tweet[text_column_name] and json_tweet['lang'] == 'en':\n",
    "                    for col in column_names:\n",
    "                        selected_row.append(json_tweet[col])\n",
    "                    data_arr.append(selected_row)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        data = pd.DataFrame(data_arr, columns=column_names)\n",
    "elif data_filepath.endswith('.csv'):\n",
    "    data = pd.read_csv(data_filepath)[column_names]\n",
    "print(data.shape)\n",
    "print(data.head(10))\n",
    "tweets = data[text_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(item):\n",
    "    item = item.lower() # convert to lowercase\n",
    "    item = \" \".join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in item.split()]) # lemmatizing\n",
    "    item = item.replace('-', ' ') # replace dashes with whitespace\n",
    "    # remove numbers, punctuation, tags and URLs\n",
    "    item = re.sub(r'[^a-zA-Z ]+|(@[A-Za-z0-9]+)|http\\S+', '', item)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     may i get your name mine be   she would pat h...\n",
       "1    move auba in the middle and bring on nelsonmar...\n",
       "2    giantlittleman im so happy for u yet so jealou...\n",
       "3                                     call me ice baby\n",
       "4    zonephysics i wonder what the co count is how ...\n",
       "5    gwrhelp it be book in advance to confirm this ...\n",
       "6    jed forest v hawick rugby live stream iphone t...\n",
       "7    cyaler beeeelzebub uncastellsmes badibulgator ...\n",
       "8    dr wilson come and get my baby for his circumc...\n",
       "9    cmclymer msnebraskajones we have video of him ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_tweets = tweets.apply(preprocess)\n",
    "preprocessed_tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['absolutely',\n",
       " 'account',\n",
       " 'action',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'adventure',\n",
       " 'adventure call',\n",
       " 'agree',\n",
       " 'aint',\n",
       " 'already',\n",
       " 'also',\n",
       " 'always',\n",
       " 'amaze',\n",
       " 'american',\n",
       " 'amp',\n",
       " 'another',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'aquarius',\n",
       " 'aries']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = vectorizer.fit_transform(preprocessed_tweets).toarray()\n",
    "# Add id column\n",
    "X = np.c_[data['id'], X]\n",
    "vectorizer.get_feature_names()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19591, 501) (6531, 501)\n",
      "                    id  absolutely  account  action  actually  add  adventure  \\\n",
      "0  1236321414216441856           0        0       0         0    0          0   \n",
      "1  1236321515643207680           0        0       0         0    0          0   \n",
      "2  1236317379866615808           0        0       0         0    0          0   \n",
      "3  1236320508741914628           0        0       0         0    0          0   \n",
      "4  1236316054563049472           0        0       0         0    0          0   \n",
      "\n",
      "   adventure call  agree  aint  ...  write  wrong  yall  yeah  year  years  \\\n",
      "0               0      0     0  ...      0      0     0     0     0      0   \n",
      "1               0      0     0  ...      0      0     0     0     0      0   \n",
      "2               0      0     0  ...      0      0     0     0     0      0   \n",
      "3               0      0     0  ...      0      0     0     0     0      0   \n",
      "4               0      0     0  ...      0      0     0     0     0      0   \n",
      "\n",
      "   yes  yet  youre  youre give  \n",
      "0    0    0      0           0  \n",
      "1    0    0      0           0  \n",
      "2    0    0      0           0  \n",
      "3    0    0      0           0  \n",
      "4    0    0      0           0  \n",
      "\n",
      "[5 rows x 501 columns]\n"
     ]
    }
   ],
   "source": [
    "# Split the matrix into random training and testing subsets\n",
    "test_size = 0.25\n",
    "train_fpath = op.join(op.dirname(data_filepath), 'train_data.csv')\n",
    "test_fpath = op.join(op.dirname(data_filepath), 'test_data.csv')\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=test_size)\n",
    "X_train = pd.DataFrame(X_train, columns=['id']+vectorizer.get_feature_names())\n",
    "X_test = pd.DataFrame(X_test, columns=['id']+vectorizer.get_feature_names())\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(X_train.head())\n",
    "# Save to csv files\n",
    "X_train.to_csv(train_fpath, index=False)\n",
    "X_test.to_csv(test_fpath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aalto-sci-project",
   "language": "python",
   "name": "aalto-sci-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
